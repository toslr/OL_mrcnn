<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; OL_mrcnn 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=eafc0fe6" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=8d563738"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            OL_mrcnn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Introduction</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#plugin-user-guide">Plugin User Guide</a><ul>
<li><a class="reference internal" href="#preprocessing">Preprocessing</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#python-user-guide">Python User Guide</a><ul>
<li><a class="reference internal" href="#data-structure">Data structure</a></li>
<li><a class="reference internal" href="#retraining-a-single-class-model">Retraining a single class model</a></li>
<li><a class="reference internal" href="#retraining-a-multi-class-model">Retraining a multi-class model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">OL_mrcnn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p>This is an introduction guide to setup and use a Fiji plugin for oligodendrocyte segmentation.</p>
</section>
<section id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h1>
<p>Prerequisites: Python 3.9, Miniconda 3, Fiji</p>
<p>Downloads required:</p>
<ul class="simple">
<li><p>model: <a class="reference external" href="https://github.com/toslr/OL_mrcnn">https://github.com/toslr/OL_mrcnn</a></p></li>
<li><p>plugin and weight files <a class="reference external" href="https://drive.google.com/drive/folders/1PIstT451WQIOS59vtHqkq8PTD-xO0Gj_?usp=sharing">here</a></p></li>
</ul>
<ol class="arabic simple">
<li><p>From a terminal, place yourself in the <code class="docutils literal notranslate"><span class="pre">OL_mrcnn-main</span></code> model folder. Create a conda environment using the .yml file:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">env</span> <span class="n">create</span> <span class="o">-</span><span class="n">f</span> <span class="n">environment</span><span class="o">.</span><span class="n">yml</span>
</pre></div>
</div>
<ol class="arabic" start="2">
<li><p>Place the model weights in the folder <code class="docutils literal notranslate"><span class="pre">/logs</span></code>. These contain the original weights (trained on COCO dataset) and the weights trained on a custom dataset. Feel free to add your own weight files.</p></li>
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">config.txt</span></code> file in the <code class="docutils literal notranslate"><span class="pre">OL_segmentation</span></code> folder and set:</p>
<blockquote>
<div><ul>
<li><p>the first line to the path to the environmental python. You can use the following command to find it and add <code class="docutils literal notranslate"><span class="pre">/bin/python</span></code> at the end of the path if not already there:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">info</span> <span class="o">--</span><span class="n">envs</span>
</pre></div>
</div>
</li>
<li><p>the second line to the path to the <code class="docutils literal notranslate"><span class="pre">OL_mrcnn-main</span></code> folder</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Move the <code class="docutils literal notranslate"><span class="pre">OL_segmentation</span></code> plugin folder inside the Fiji plugins. Restart Fiji to take into account the changes.</p></li>
</ol>
</section>
<section id="plugin-user-guide">
<h1>Plugin User Guide<a class="headerlink" href="#plugin-user-guide" title="Link to this heading"></a></h1>
<section id="preprocessing">
<h2>Preprocessing<a class="headerlink" href="#preprocessing" title="Link to this heading"></a></h2>
<p>Before starting your analyses, your images need to be readable. Launch the “Preprocess images” options of the Fiji plugin. A window should pop.</p>
<ul class="simple">
<li><p>Select the folder you want to preprocess.</p></li>
<li><p>Select the type of your images.</p></li>
<li><p>Select the channels you want to take into account. This choice will affect the accuracy of the results. Below is an example of which channels give the best results.</p></li>
</ul>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="_images/wrong_channel_select.png"><img alt="alternative text" src="_images/wrong_channel_select.png" style="width: 360px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-text">Selecting only green and blue here will lead to bad results (regardless of contrast).</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="_images/right_channel_select.png"><img alt="alternative text" src="_images/right_channel_select.png" style="width: 360px; height: 300px;" /></a>
<figcaption>
<p><span class="caption-text">Better to select red and blue / red and green and blue.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Select whether you want RGB or grayscale images. RGB is recommended for the analysis.</p></li>
<li><p>Hit “Ok”. Your new folder of images will appear next to the former one under the name “oldername_norm”</p></li>
</ul>
</section>
<section id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Link to this heading"></a></h2>
<p>Select the “Run MRCNN” option of the Fiji plugin. A window should pop.</p>
<ul class="simple">
<li><p>Select the task you want to achieve. <strong>The segmentation part is still under development. Please refer to the Python userguide for segmenting.</strong></p></li>
<li><p>Choose the dataset you want to process.</p></li>
<li><p>Select the weight file you want to use. Click Other to choose your own.</p></li>
<li><p>Choose a name for the task. it will be the name of the results folder.</p></li>
<li><p>Adjust the confidence and non-maximum suppression thresholds.</p></li>
<li><p>Choose whether you want to visualize and edit the results,</p></li>
<li><p>Choose if you want to save the crops.</p></li>
<li><p>Hit “Ok”.</p></li>
</ul>
<p>If the results editor has been selected, the images will be displayed along with the ROIs. You can navigate between the images, edit, add or delete the ROIs. Click ‘Finish’ to save the changes.</p>
<p>The final results of your job will be saved under the folder <code class="docutils literal notranslate"><span class="pre">results/jobName</span></code>.</p>
</section>
</section>
<section id="python-user-guide">
<h1>Python User Guide<a class="headerlink" href="#python-user-guide" title="Link to this heading"></a></h1>
<ul>
<li><p>Add your dataset in the folder <code class="docutils literal notranslate"><span class="pre">/data</span></code></p></li>
<li><p>OPTIONAL: preprocess your data with the <code class="docutils literal notranslate"><span class="pre">preprocessing.ipynb</span></code> notebook</p></li>
<li><p>Configure the <code class="docutils literal notranslate"><span class="pre">image_cropper.ipynb</span></code> notebook:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE</span></code>: device to use for inference. Default value is ‘cpu:0’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">detection_min_confidence</span></code>: minimum confidence level for the detections. Default value is 0.7.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">detection_nms_threshold</span></code>: non-maximum suppression threshold. Eliminates the least confident detection when the IoU of 2 masks is above this value. Default value is 0.3.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weights_subpath</span></code>: subpath in the <cite>/logs</cite> folder to the weights file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">results_name</span></code>: name of the folder where the results will be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_dir</span></code>: name of the folder where the images are stored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_gpu</span></code>: number of GPUs to use for inference. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_img_per_gpu</span></code>: number of images to process in parallel on each GPU. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VISUALIZE</span></code>: if True, displays the images with the detections. Default value is False.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Run the notebook. The results will be saved in the folder <code class="docutils literal notranslate"><span class="pre">/results/results_name</span></code>.</p></li>
</ul>
<ul>
<li><p>In this setup, we run a first model to crop and classify objects in the images. Then we run a second model on the cropped images to get a refined mask.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">model_pipeline.ipynb</span></code> notebook, configure the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE</span></code>: device to use for inference. Default value is ‘cpu:0’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_count_macro</span></code>: number of GPUs to use for the first model. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_img_per_gpu_macro</span></code>: number of images to process in parallel on each GPU for the first model. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_confidence_macro</span></code>: minimum confidence level for the detections in the first model. Default value is 0.7.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nms_threshold_macro</span></code>: non-maximum suppression threshold for the first model. Default value is 0.3.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nms_multiclass_macro</span></code>: non-maximum suppression threshold between classes for the first model. Default value is 0.3.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_count_micro</span></code>: number of GPUs to use for the second model. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_img_per_gpu_micro</span></code>: number of images to process in parallel on each GPU for the second model. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_confidence_micro</span></code>: minimum confidence level for the detections in the second model. Default value is 0.7.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nms_threshold_micro</span></code>: non-maximum suppression threshold for the second model. Default value is 0.3.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MACRO_MODEL_SUBPATH</span></code>: subpath in the <cite>/logs</cite> folder to the weights file of the first model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MICRO_MODEL_SUBPATH</span></code>: subpath in the <cite>/logs</cite> folder to the weights file of the second model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RESULTS_NAME</span></code>: name of the folder where the results will be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TEST_DIR</span></code>: name of the folder where the images are stored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VISUALIZE</span></code>: if True, displays the images with the detections. Default value is False.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Run the notebook. The results will be saved in the folder <code class="docutils literal notranslate"><span class="pre">/results/RESULTS_NAME</span></code>.</p></li>
</ul>
<section id="data-structure">
<h2>Data structure<a class="headerlink" href="#data-structure" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">/data</span></code> folder in the root directory.</p></li>
<li><p>Inside the <code class="docutils literal notranslate"><span class="pre">/data</span></code> directory, put your images in a folder named <code class="docutils literal notranslate"><span class="pre">/imgs</span></code> and your binary masks in a folder named <code class="docutils literal notranslate"><span class="pre">/masks</span></code>. The name, size and format of the masks must match the images.</p></li>
<li><p>In the <a href="#id1"><span class="problematic" id="id2">``</span></a>roi_labels_to_json.py``script, configure the <a href="#id3"><span class="problematic" id="id4">``</span></a>dir_path``in the <cite>main()</cite> function. Run in a terminal:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">roi_labels_to_json</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Move the label files to a <code class="docutils literal notranslate"><span class="pre">jsons</span></code> folder in the <a href="#id5"><span class="problematic" id="id6">``</span></a>/data``directory.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">format_data.py</span></code> script, configure the <code class="docutils literal notranslate"><span class="pre">dir_path</span></code> in the <cite>main()</cite> function. Configure the size the of the training / validation / test datasets (usually 0.6, 0.2, 0.2) Run in a terminal:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">format_data</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="retraining-a-single-class-model">
<h2>Retraining a single class model<a class="headerlink" href="#retraining-a-single-class-model" title="Link to this heading"></a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>In the <code class="docutils literal notranslate"><span class="pre">custom.py</span></code> script, configure the following:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">GRAYSCALE</span></code>: if True, the model will be trained on grayscale images. Default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code>: path to the dataset. Default value is ‘/data’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NAME</span></code>: name of the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GPU_COUNT</span></code>: number of GPUs to use. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">IMAGES_PER_GPU</span></code>: number of images to process in parallel on each GPU. Default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NUM_CLASSES</span></code>: number of classes. Default value is 2.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EPOCHS</span></code>: number of epochs. Default value is 50.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">STEPS</span> <span class="pre">PER</span> <span class="pre">EPOCH</span></code>: number of steps per epoch. Default value is 50.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LEARNING_RATE</span></code>: learning rate. Default value is 0.001.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LAYERS</span></code>: layers to train. Default value is ‘heads’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DETECTION_MIN_CONFIDENCE</span></code>: minimum confidence level for the detections. Default value is 0.7.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE</span></code>: device to use for training. Default value is ‘cpu:0’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MAX_GT_INSTANCES</span></code>: maximum number of instances in the ground truth. Default value is 100.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DETECTION_MAX_INSTANCES</span></code>: maximum number of instances in the detections. Default value is 35.</p></li>
<li><p>in the <a href="#id7"><span class="problematic" id="id8">``</span></a>CustomDataset``class, modify or add lines :</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Run the script in a terminal:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">custom</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="retraining-a-multi-class-model">
<h2>Retraining a multi-class model<a class="headerlink" href="#retraining-a-multi-class-model" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Same instructions as before but on the <code class="docutils literal notranslate"><span class="pre">custom_multi.py</span></code> script.</p></li>
<li><p>Run the script in a terminal:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">custom_multi</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Tom Soulaire.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>