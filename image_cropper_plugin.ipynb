{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creates a session with device placement logs\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "import tifffile as tiff\n",
    "import argparse\n",
    "\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn.visualize import display_instances\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "from postprocessing import nms_suppression_multi, crop_from_csv, crop_from_results\n",
    "\n",
    "def main():\n",
    "\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) #os.getcwd()\n",
    "    sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "    DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', type=str, default='/cpu:0', help='Device to use')\n",
    "    parser.add_argument('--g', type=int, default=1, help='Number of GPUs to use')\n",
    "    parser.add_argument('--batch', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--ci', type=float, default=0.7, help='Minimum confidence level for detection')\n",
    "    parser.add_argument('--nms', type=float, default=0.3, help='NMS threshold for detection')\n",
    "    parser.add_argument('--weights', type=str, default='mask_rcnn_coco.h5', help='Subpath to weights file')\n",
    "    parser.add_argument('--name', type=str, default='results0', help='Name of results directory')\n",
    "    parser.add_argument('--data', type=str, default='data/test/imgs', help='Directory containing test images')\n",
    "    parser.add_argument('--gray', type=bool, default=False, help='Whether to convert images to grayscale')\n",
    "    parser.add_argument('--edit', type=bool, default=False, help='Whether to visualize and edit the results')\n",
    "    parser.add_argument('--multiclass', type=bool, default=False, help='Whether to use the multiclass model')\n",
    "    parser.add_argument('--save', type=bool, default=True, help='Whether to save the crops')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    TEST_DIR = args.data\n",
    "    GRAYSCALE = args.gray\n",
    "    if args.multiclass:\n",
    "        from custom_multi import CustomConfig, CustomDataset\n",
    "    else:\n",
    "        from custom import CustomConfig, CustomDataset\n",
    "\n",
    "    class InferenceConfig(CustomConfig):\n",
    "        DEVICE = args.device\n",
    "        GPU_COUNT = args.g\n",
    "        IMAGES_PER_GPU = args.batch\n",
    "        DETECTION_MIN_CONFIDENCE = args.ci #Minimum probability value to accept a detected instance\n",
    "        DETECTION_NMS_THRESHOLD = args.nms # Non-maximum suppression threshold for detection\n",
    "\n",
    "    inference_config = InferenceConfig()\n",
    "    macro_model = modellib.MaskRCNN(mode=\"inference\",\n",
    "                                config=inference_config,\n",
    "                                model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "    macro_model_path = os.path.join(DEFAULT_LOGS_DIR, args.weights)\n",
    "\n",
    "    print(\"Loading macro weights from \", macro_model_path)\n",
    "    tf.keras.Model.load_weights(macro_model.keras_model, macro_model_path , by_name=True, skip_mismatch=True)\n",
    "    RESULTS_NAME = args.name\n",
    "    RESULTS_DIR = os.path.join(ROOT_DIR, \"results\", RESULTS_NAME)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = CustomDataset()\n",
    "    dataset_val.load_custom(os.path.join(ROOT_DIR,\"data\"), \"valid\")\n",
    "    dataset_val.prepare()\n",
    "    image_paths = []\n",
    "    for filename in sorted(os.listdir(TEST_DIR)):\n",
    "        if filename.endswith(\".tif\"):\n",
    "            image_paths.append(os.path.join(TEST_DIR, filename))\n",
    "\n",
    "    res_list = []\n",
    "    for image_path in image_paths:\n",
    "        print(image_path)\n",
    "        original_img = skimage.io.imread(image_path)\n",
    "        # PREPROCESSING STEP TO GET IMAGES IN THE EXPECTED FORMAT\n",
    "        if GRAYSCALE:\n",
    "            float_gray_img = skimage.io.imread(image_path, as_gray=True)\n",
    "            float_gray_img_norm = skimage.exposure.equalize_adapthist(float_gray_img, clip_limit=0.02)\n",
    "            img_norm = np.stack(((float_gray_img_norm * 255).astype(np.uint8),)*3, axis=-1)\n",
    "        else:\n",
    "            norm_channels = []\n",
    "            for i in range(original_img.shape[2]):\n",
    "                if np.max(original_img[:,:,i]) > 0:\n",
    "                    norm_channel = skimage.exposure.equalize_adapthist(original_img[:,:,i], clip_limit=0.02)\n",
    "                    norm_channel = (norm_channel * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    norm_channel = original_img[:,:,i]\n",
    "                norm_channels.append(norm_channel)\n",
    "            img_norm = np.stack(norm_channels, axis=-1)\n",
    "\n",
    "        macro_results = macro_model.detect([img_norm], verbose=0)\n",
    "        macro_results = nms_suppression_multi(macro_results, args.nms)\n",
    "        r = macro_results[0]\n",
    "        print(len(r['rois']))\n",
    "        for i in range(len(r['rois'])):\n",
    "            res_list.append([os.path.basename(image_path), \n",
    "                             len(res_list)+1, \n",
    "                             dataset_val.class_names[r['class_ids'][i]], \n",
    "                             r['scores'][i],\n",
    "                             r['rois'][i]])\n",
    "\n",
    "    res_df = pd.DataFrame(res_list, columns=['image_name', 'detection_id', 'class', 'score', 'bbox'])\n",
    "    res_df.to_csv(os.path.join(RESULTS_DIR, 'results.csv'), index=False)\n",
    "    crop_from_csv(os.path.join(RESULTS_DIR, 'results.csv'), TEST_DIR, RESULTS_DIR)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
